
<!doctype html>
<html lang="en" class="no-js">
  <head>
    
      <meta charset="utf-8">
      <meta name="viewport" content="width=device-width,initial-scale=1">
      
      
      
        <link rel="canonical" href="https://docs.mlcommons.org/inference/benchmarks/language/bert/">
      
      
        <link rel="prev" href="../../medical_imaging/3d-unet/">
      
      
        <link rel="next" href="../gpt-j/">
      
      
      <link rel="icon" href="../../../img/logo_v2.svg">
      <meta name="generator" content="mkdocs-1.6.1, mkdocs-material-9.6.20">
    
    
      
        <title>Bert-Large - MLPerf Inference Documentation</title>
      
    
    
      <link rel="stylesheet" href="../../../assets/stylesheets/main.e53b48f4.min.css">
      
        
        <link rel="stylesheet" href="../../../assets/stylesheets/palette.06af60db.min.css">
      
      


    
    
      
    
    
      
        
        
        <link rel="preconnect" href="https://fonts.gstatic.com" crossorigin>
        <link rel="stylesheet" href="https://fonts.googleapis.com/css?family=Roboto:300,300i,400,400i,700,700i%7CRoboto+Mono:400,400i,700,700i&display=fallback">
        <style>:root{--md-text-font:"Roboto";--md-code-font:"Roboto Mono"}</style>
      
    
    
    <script>__md_scope=new URL("../../..",location),__md_hash=e=>[...e].reduce(((e,_)=>(e<<5)-e+_.charCodeAt(0)),0),__md_get=(e,_=localStorage,t=__md_scope)=>JSON.parse(_.getItem(t.pathname+"."+e)),__md_set=(e,_,t=localStorage,a=__md_scope)=>{try{t.setItem(a.pathname+"."+e,JSON.stringify(_))}catch(e){}}</script>
    
      

    
    
    
  </head>
  
  
    
    
    
    
    
    <body dir="ltr" data-md-color-scheme="default" data-md-color-primary="deep-purple" data-md-color-accent="green">
  
    
    <input class="md-toggle" data-md-toggle="drawer" type="checkbox" id="__drawer" autocomplete="off">
    <input class="md-toggle" data-md-toggle="search" type="checkbox" id="__search" autocomplete="off">
    <label class="md-overlay" for="__drawer"></label>
    <div data-md-component="skip">
      
        
        <a href="#question-answering-using-bert-large" class="md-skip">
          Skip to content
        </a>
      
    </div>
    <div data-md-component="announce">
      
    </div>
    
    
      

  

<header class="md-header md-header--shadow md-header--lifted" data-md-component="header">
  <nav class="md-header__inner md-grid" aria-label="Header">
    <a href="../../.." title="MLPerf Inference Documentation" class="md-header__button md-logo" aria-label="MLPerf Inference Documentation" data-md-component="logo">
      
  <img src="../../../img/logo_v2.svg" alt="logo">

    </a>
    <label class="md-header__button md-icon" for="__drawer">
      
      <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M3 6h18v2H3zm0 5h18v2H3zm0 5h18v2H3z"/></svg>
    </label>
    <div class="md-header__title" data-md-component="header-title">
      <div class="md-header__ellipsis">
        <div class="md-header__topic">
          <span class="md-ellipsis">
            MLPerf Inference Documentation
          </span>
        </div>
        <div class="md-header__topic" data-md-component="header-topic">
          <span class="md-ellipsis">
            
              Bert-Large
            
          </span>
        </div>
      </div>
    </div>
    
      
    
    
    
    
      
      
        <label class="md-header__button md-icon" for="__search">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        </label>
        <div class="md-search" data-md-component="search" role="dialog">
  <label class="md-search__overlay" for="__search"></label>
  <div class="md-search__inner" role="search">
    <form class="md-search__form" name="search">
      <input type="text" class="md-search__input" name="query" aria-label="Search" placeholder="Search" autocapitalize="off" autocorrect="off" autocomplete="off" spellcheck="false" data-md-component="search-query" required>
      <label class="md-search__icon md-icon" for="__search">
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M9.5 3A6.5 6.5 0 0 1 16 9.5c0 1.61-.59 3.09-1.56 4.23l.27.27h.79l5 5-1.5 1.5-5-5v-.79l-.27-.27A6.52 6.52 0 0 1 9.5 16 6.5 6.5 0 0 1 3 9.5 6.5 6.5 0 0 1 9.5 3m0 2C7 5 5 7 5 9.5S7 14 9.5 14 14 12 14 9.5 12 5 9.5 5"/></svg>
        
        <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M20 11v2H8l5.5 5.5-1.42 1.42L4.16 12l7.92-7.92L13.5 5.5 8 11z"/></svg>
      </label>
      <nav class="md-search__options" aria-label="Search">
        
        <button type="reset" class="md-search__icon md-icon" title="Clear" aria-label="Clear" tabindex="-1">
          
          <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M19 6.41 17.59 5 12 10.59 6.41 5 5 6.41 10.59 12 5 17.59 6.41 19 12 13.41 17.59 19 19 17.59 13.41 12z"/></svg>
        </button>
      </nav>
      
    </form>
    <div class="md-search__output">
      <div class="md-search__scrollwrap" tabindex="0" data-md-scrollfix>
        <div class="md-search-result" data-md-component="search-result">
          <div class="md-search-result__meta">
            Initializing search
          </div>
          <ol class="md-search-result__list" role="presentation"></ol>
        </div>
      </div>
    </div>
  </div>
</div>
      
    
    
      <div class="md-header__source">
        <a href="https://github.com/mlcommons/inference" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
      </div>
    
  </nav>
  
    
      
<nav class="md-tabs" aria-label="Tabs" data-md-component="tabs">
  <div class="md-grid">
    <ul class="md-tabs__list">
      
        
  
  
  
    
  
  
    
    
      <li class="md-tabs__item md-tabs__item--active">
        <a href="../../.." class="md-tabs__link">
          
  
  
    
  
  Home

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../install/" class="md-tabs__link">
          
  
  
    
  
  Install MLCFlow

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../submission/" class="md-tabs__link">
          
  
  
    
  
  Submission

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../power/" class="md-tabs__link">
          
  
  
    
  
  Power

        </a>
      </li>
    
  

      
        
  
  
  
  
    
    
      <li class="md-tabs__item">
        <a href="../../../changelog/" class="md-tabs__link">
          
  
  
    
  
  Release Notes

        </a>
      </li>
    
  

      
    </ul>
  </div>
</nav>
    
  
</header>
    
    <div class="md-container" data-md-component="container">
      
      
        
      
      <main class="md-main" data-md-component="main">
        <div class="md-main__inner md-grid">
          
            
              
                
              
              <div class="md-sidebar md-sidebar--primary" data-md-component="sidebar" data-md-type="navigation" >
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    


  


<nav class="md-nav md-nav--primary md-nav--lifted" aria-label="Navigation" data-md-level="0">
  <label class="md-nav__title" for="__drawer">
    <a href="../../.." title="MLPerf Inference Documentation" class="md-nav__button md-logo" aria-label="MLPerf Inference Documentation" data-md-component="logo">
      
  <img src="../../../img/logo_v2.svg" alt="logo">

    </a>
    MLPerf Inference Documentation
  </label>
  
    <div class="md-nav__source">
      <a href="https://github.com/mlcommons/inference" title="Go to repository" class="md-source" data-md-component="source">
  <div class="md-source__icon md-icon">
    
    <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 448 512"><!--! Font Awesome Free 7.0.0 by @fontawesome - https://fontawesome.com License - https://fontawesome.com/license/free (Icons: CC BY 4.0, Fonts: SIL OFL 1.1, Code: MIT License) Copyright 2025 Fonticons, Inc.--><path d="M439.6 236.1 244 40.5c-5.4-5.5-12.8-8.5-20.4-8.5s-15 3-20.4 8.4L162.5 81l51.5 51.5c27.1-9.1 52.7 16.8 43.4 43.7l49.7 49.7c34.2-11.8 61.2 31 35.5 56.7-26.5 26.5-70.2-2.9-56-37.3L240.3 199v121.9c25.3 12.5 22.3 41.8 9.1 55-6.4 6.4-15.2 10.1-24.3 10.1s-17.8-3.6-24.3-10.1c-17.6-17.6-11.1-46.9 11.2-56v-123c-20.8-8.5-24.6-30.7-18.6-45L142.6 101 8.5 235.1C3 240.6 0 247.9 0 255.5s3 15 8.5 20.4l195.6 195.7c5.4 5.4 12.7 8.4 20.4 8.4s15-3 20.4-8.4l194.7-194.7c5.4-5.4 8.4-12.8 8.4-20.4s-3-15-8.4-20.4"/></svg>
  </div>
  <div class="md-source__repository">
    GitHub
  </div>
</a>
    </div>
  
  <ul class="md-nav__list" data-md-scrollfix>
    
      
      
  
  
    
  
  
  
    
    
      
        
          
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
        
        
      
      
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1" checked>
        
          
          <div class="md-nav__link md-nav__container">
            <a href="../../.." class="md-nav__link ">
              
  
  
  <span class="md-ellipsis">
    Home
    
  </span>
  

            </a>
            
              
              <label class="md-nav__link " for="__nav_1" id="__nav_1_label" tabindex="">
                <span class="md-nav__icon md-icon"></span>
              </label>
            
          </div>
        
        <nav class="md-nav" data-md-level="1" aria-labelledby="__nav_1_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1">
            <span class="md-nav__icon md-icon"></span>
            Home
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_2" >
        
          
          <label class="md-nav__link" for="__nav_1_2" id="__nav_1_2_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Image Classification
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_2_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_2">
            <span class="md-nav__icon md-icon"></span>
            Image Classification
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../image_classification/resnet50/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    ResNet50
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_3" >
        
          
          <label class="md-nav__link" for="__nav_1_3" id="__nav_1_3_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Text to Image
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_3_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_3">
            <span class="md-nav__icon md-icon"></span>
            Text to Image
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../text_to_image/sdxl/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Stable Diffusion
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_4" >
        
          
          <label class="md-nav__link" for="__nav_1_4" id="__nav_1_4_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    2D Object Detection
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_4_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_4">
            <span class="md-nav__icon md-icon"></span>
            2D Object Detection
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../object_detection/retinanet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    RetinaNet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_5" >
        
          
          <label class="md-nav__link" for="__nav_1_5" id="__nav_1_5_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Automotive
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_5_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_5">
            <span class="md-nav__icon md-icon"></span>
            Automotive
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../automotive/3d_object_detection/pointpainting/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    3D Object Detection
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_6" >
        
          
          <label class="md-nav__link" for="__nav_1_6" id="__nav_1_6_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Medical Imaging
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_6_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_6">
            <span class="md-nav__icon md-icon"></span>
            Medical Imaging
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../medical_imaging/3d-unet/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    3d-unet
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
    
  
  
  
    
    
      
        
      
        
      
        
      
        
      
        
      
        
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--active md-nav__item--section md-nav__item--nested">
      
        
        
        <input class="md-nav__toggle md-toggle " type="checkbox" id="__nav_1_7" checked>
        
          
          <label class="md-nav__link" for="__nav_1_7" id="__nav_1_7_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Language Processing
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_7_label" aria-expanded="true">
          <label class="md-nav__title" for="__nav_1_7">
            <span class="md-nav__icon md-icon"></span>
            Language Processing
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
    
  
  
  
    <li class="md-nav__item md-nav__item--active">
      
      <input class="md-nav__toggle md-toggle" type="checkbox" id="__toc">
      
      
        
      
      
        <label class="md-nav__link md-nav__link--active" for="__toc">
          
  
  
  <span class="md-ellipsis">
    Bert-Large
    
  </span>
  

          <span class="md-nav__icon md-icon"></span>
        </label>
      
      <a href="./" class="md-nav__link md-nav__link--active">
        
  
  
  <span class="md-ellipsis">
    Bert-Large
    
  </span>
  

      </a>
      
        

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mlperf-reference-implementation-in-python" class="md-nav__link">
    <span class="md-ellipsis">
      MLPerf Reference Implementation in Python
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MLPerf Reference Implementation in Python">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datacenter-category" class="md-nav__link">
    <span class="md-ellipsis">
      Datacenter category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Datacenter category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch-framework" class="md-nav__link">
    <span class="md-ellipsis">
      Pytorch framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pytorch framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_1" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_1" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_1" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_2" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_2" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_1" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_3" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_3" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_1" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda-device" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_1" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_1" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_2" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_4" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_4" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_2" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_5" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_5" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_2" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_1" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_1" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_1" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_3" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_6" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_6" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_3" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_7" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_7" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_3" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rocm-device" class="md-nav__link">
    <span class="md-ellipsis">
      ROCm device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ROCm device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#native-environment_2" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_2" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_2" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_4" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_8" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_8" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_4" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_9" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_9" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_4" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepsparse-framework" class="md-nav__link">
    <span class="md-ellipsis">
      Deepsparse framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deepsparse framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_1" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_2" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_2" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_5" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_10" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_10" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_5" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_11" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_11" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_5" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_3" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_3" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_3" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_6" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_12" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_12" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_6" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_13" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_13" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_6" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#edge-category" class="md-nav__link">
    <span class="md-ellipsis">
      Edge category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Edge category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch-framework_1" class="md-nav__link">
    <span class="md-ellipsis">
      Pytorch framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pytorch framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_2" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_3" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_3" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_7" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_14" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_14" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_15" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_15" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_7" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_4" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_4" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_4" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_8" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_16" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_16" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_1" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_17" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_17" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_8" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda-device_1" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_4" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_4" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_9" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_18" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_18" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_2" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_19" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_19" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_9" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_5" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_5" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_5" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_10" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_20" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_20" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_3" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_21" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_21" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_10" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rocm-device_1" class="md-nav__link">
    <span class="md-ellipsis">
      ROCm device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ROCm device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#native-environment_6" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_6" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_6" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_11" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_22" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_22" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_4" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_23" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_23" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_11" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepsparse-framework_1" class="md-nav__link">
    <span class="md-ellipsis">
      Deepsparse framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deepsparse framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_3" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_5" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_5" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_12" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_24" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_24" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_5" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_25" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_25" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_12" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_7" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_7" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_7" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_13" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_26" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_26" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_6" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_27" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_27" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_13" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datacenter-category_1" class="md-nav__link">
    <span class="md-ellipsis">
      Datacenter category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Datacenter category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch-framework_2" class="md-nav__link">
    <span class="md-ellipsis">
      Pytorch framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pytorch framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_4" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_6" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_8" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_14" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_28" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_28" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_7" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_29" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_29" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_14" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_8" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_9" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_15" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_30" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_30" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_8" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_31" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_31" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_15" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda-device_2" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_7" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_10" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_16" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_32" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_32" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_9" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_33" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_33" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_16" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_9" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_11" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_17" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_34" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_34" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_10" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_35" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_35" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_17" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rocm-device_2" class="md-nav__link">
    <span class="md-ellipsis">
      ROCm device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ROCm device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#native-environment_10" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_12" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_18" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_36" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_36" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_11" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_37" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_37" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_18" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepsparse-framework_2" class="md-nav__link">
    <span class="md-ellipsis">
      Deepsparse framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deepsparse framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_5" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_8" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_13" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_19" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_38" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_38" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_12" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_39" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_39" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_19" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_11" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_14" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_20" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_40" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_40" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_13" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_41" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_41" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_20" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nvidia-mlperf-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Nvidia MLPerf Implementation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Nvidia MLPerf Implementation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datacenter-category_2" class="md-nav__link">
    <span class="md-ellipsis">
      Datacenter category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Datacenter category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorrt-framework" class="md-nav__link">
    <span class="md-ellipsis">
      TensorRT framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TensorRT framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda-device_3" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_9" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_6" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_21" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_42" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_42" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_14" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_43" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_43" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_21" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#edge-category_1" class="md-nav__link">
    <span class="md-ellipsis">
      Edge category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Edge category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorrt-framework_1" class="md-nav__link">
    <span class="md-ellipsis">
      TensorRT framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TensorRT framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda-device_4" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_10" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_7" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_22" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_44" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_44" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_7" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_45" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_45" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_22" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datacenter-category_3" class="md-nav__link">
    <span class="md-ellipsis">
      Datacenter category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Datacenter category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorrt-framework_2" class="md-nav__link">
    <span class="md-ellipsis">
      TensorRT framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TensorRT framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda-device_5" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_11" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_15" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_23" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_46" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_46" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_15" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_47" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_47" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_23" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../gpt-j/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    GPT-J
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../llama2-70b/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    LLAMA2-70B
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama3_1-405b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLAMA3-405B
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../llama3_1-8b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    LLAMA3-8B
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../mixtral-8x7b/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    MIXTRAL-8x7B
    
  </span>
  

      </a>
    </li>
  

              
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../deepseek-r1/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DeepSeek-R1
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_8" >
        
          
          <label class="md-nav__link" for="__nav_1_8" id="__nav_1_8_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Recommendation
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_8_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_8">
            <span class="md-nav__icon md-icon"></span>
            Recommendation
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../recommendation/dlrm-v2/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    DLRM-v2
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_9" >
        
          
          <label class="md-nav__link" for="__nav_1_9" id="__nav_1_9_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Graph Neural Networks
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_9_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_9">
            <span class="md-nav__icon md-icon"></span>
            Graph Neural Networks
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../graph/rgat/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    R-GAT
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
              
                
  
  
  
  
    
    
      
        
      
    
    
    
      
      
        
          
          
        
      
    
    
      
    
    <li class="md-nav__item md-nav__item--section md-nav__item--nested">
      
        
        
          
        
        <input class="md-nav__toggle md-toggle md-toggle--indeterminate" type="checkbox" id="__nav_1_10" >
        
          
          <label class="md-nav__link" for="__nav_1_10" id="__nav_1_10_label" tabindex="">
            
  
  
  <span class="md-ellipsis">
    Speech to Text
    
  </span>
  

            <span class="md-nav__icon md-icon"></span>
          </label>
        
        <nav class="md-nav" data-md-level="2" aria-labelledby="__nav_1_10_label" aria-expanded="false">
          <label class="md-nav__title" for="__nav_1_10">
            <span class="md-nav__icon md-icon"></span>
            Speech to Text
          </label>
          <ul class="md-nav__list" data-md-scrollfix>
            
              
                
  
  
  
  
    <li class="md-nav__item">
      <a href="../../speech_to_text/whisper/" class="md-nav__link">
        
  
  
  <span class="md-ellipsis">
    Whisper
    
  </span>
  

      </a>
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

              
            
          </ul>
        </nav>
      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../install/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Install MLCFlow
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../submission/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Submission
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../power/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Power
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
      
      
  
  
  
  
    
    
      
        
          
        
      
        
      
    
    
    
      
      
        
      
    
    
      
        
        
      
    
    <li class="md-nav__item md-nav__item--pruned md-nav__item--nested">
      
        
  
  
  
    <a href="../../../changelog/" class="md-nav__link">
      
  
  
  <span class="md-ellipsis">
    Release Notes
    
  </span>
  

      
        <span class="md-nav__icon md-icon"></span>
      
    </a>
  

      
    </li>
  

    
  </ul>
</nav>
                  </div>
                </div>
              </div>
            
            
              
                
              
              <div class="md-sidebar md-sidebar--secondary" data-md-component="sidebar" data-md-type="toc" hidden>
                <div class="md-sidebar__scrollwrap">
                  <div class="md-sidebar__inner">
                    

<nav class="md-nav md-nav--secondary" aria-label="Table of contents">
  
  
  
    
  
  
    <label class="md-nav__title" for="__toc">
      <span class="md-nav__icon md-icon"></span>
      Table of contents
    </label>
    <ul class="md-nav__list" data-md-component="toc" data-md-scrollfix>
      
        <li class="md-nav__item">
  <a href="#mlperf-reference-implementation-in-python" class="md-nav__link">
    <span class="md-ellipsis">
      MLPerf Reference Implementation in Python
    </span>
  </a>
  
    <nav class="md-nav" aria-label="MLPerf Reference Implementation in Python">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datacenter-category" class="md-nav__link">
    <span class="md-ellipsis">
      Datacenter category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Datacenter category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch-framework" class="md-nav__link">
    <span class="md-ellipsis">
      Pytorch framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pytorch framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_1" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_1" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_1" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_2" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_2" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_1" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_3" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_3" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_1" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda-device" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_1" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_1" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_2" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_4" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_4" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_2" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_5" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_5" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_2" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_1" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_1" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_1" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_3" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_6" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_6" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_3" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_7" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_7" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_3" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rocm-device" class="md-nav__link">
    <span class="md-ellipsis">
      ROCm device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ROCm device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#native-environment_2" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_2" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_2" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_4" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_8" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_8" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_4" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_9" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_9" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_4" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepsparse-framework" class="md-nav__link">
    <span class="md-ellipsis">
      Deepsparse framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deepsparse framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_1" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_2" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_2" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_5" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_10" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_10" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_5" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_11" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_11" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_5" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_3" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_3" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_3" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_6" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_12" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_12" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_6" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_13" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_13" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_6" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#edge-category" class="md-nav__link">
    <span class="md-ellipsis">
      Edge category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Edge category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch-framework_1" class="md-nav__link">
    <span class="md-ellipsis">
      Pytorch framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pytorch framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_2" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_3" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_3" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_7" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_14" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_14" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_15" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_15" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_7" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_4" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_4" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_4" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_8" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_16" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_16" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_1" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_17" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_17" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_8" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda-device_1" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_4" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_4" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_9" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_18" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_18" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_2" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_19" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_19" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_9" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_5" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_5" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_5" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_10" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_20" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_20" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_3" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_21" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_21" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_10" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rocm-device_1" class="md-nav__link">
    <span class="md-ellipsis">
      ROCm device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ROCm device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#native-environment_6" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_6" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_6" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_11" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_22" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_22" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_4" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_23" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_23" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_11" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepsparse-framework_1" class="md-nav__link">
    <span class="md-ellipsis">
      Deepsparse framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deepsparse framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_3" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_5" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_5" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_12" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_24" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_24" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_5" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_25" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_25" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_12" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_7" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#setup-a-virtual-environment-for-python_7" class="md-nav__link">
    <span class="md-ellipsis">
      # Setup a virtual environment for Python
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_7" class="md-nav__link">
    <span class="md-ellipsis">
      # Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_13" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_26" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_26" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_6" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_27" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_27" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_13" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datacenter-category_1" class="md-nav__link">
    <span class="md-ellipsis">
      Datacenter category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Datacenter category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#pytorch-framework_2" class="md-nav__link">
    <span class="md-ellipsis">
      Pytorch framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Pytorch framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_4" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_6" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_8" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_14" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_28" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_28" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_7" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_29" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_29" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_14" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_8" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_9" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_15" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_30" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_30" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_8" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_31" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_31" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_15" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#cuda-device_2" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_7" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_10" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_16" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_32" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_32" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_9" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_33" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_33" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_16" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_9" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_11" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_17" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_34" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_34" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_10" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_35" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_35" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_17" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#rocm-device_2" class="md-nav__link">
    <span class="md-ellipsis">
      ROCm device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="ROCm device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#native-environment_10" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_12" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_18" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_36" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_36" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_11" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_37" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_37" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_18" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#deepsparse-framework_2" class="md-nav__link">
    <span class="md-ellipsis">
      Deepsparse framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Deepsparse framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cpu-device_5" class="md-nav__link">
    <span class="md-ellipsis">
      CPU device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CPU device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_8" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_13" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_19" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_38" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_38" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_12" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_39" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_39" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_19" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#native-environment_11" class="md-nav__link">
    <span class="md-ellipsis">
      Native Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_14" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_20" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_40" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_40" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_13" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_41" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_41" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_20" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
        <li class="md-nav__item">
  <a href="#nvidia-mlperf-implementation" class="md-nav__link">
    <span class="md-ellipsis">
      Nvidia MLPerf Implementation
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Nvidia MLPerf Implementation">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#datacenter-category_2" class="md-nav__link">
    <span class="md-ellipsis">
      Datacenter category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Datacenter category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorrt-framework" class="md-nav__link">
    <span class="md-ellipsis">
      TensorRT framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TensorRT framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda-device_3" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_9" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_6" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_21" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_42" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_42" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_14" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_43" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_43" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_21" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#edge-category_1" class="md-nav__link">
    <span class="md-ellipsis">
      Edge category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Edge category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorrt-framework_1" class="md-nav__link">
    <span class="md-ellipsis">
      TensorRT framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TensorRT framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda-device_4" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_10" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#docker-container-build-and-performance-estimation-for-offline-scenario_7" class="md-nav__link">
    <span class="md-ellipsis">
      # Docker Container Build and Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_22" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_44" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_44" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#singlestream_7" class="md-nav__link">
    <span class="md-ellipsis">
      SingleStream
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_45" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_45" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_22" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
          <li class="md-nav__item">
  <a href="#datacenter-category_3" class="md-nav__link">
    <span class="md-ellipsis">
      Datacenter category
    </span>
  </a>
  
    <nav class="md-nav" aria-label="Datacenter category">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#tensorrt-framework_2" class="md-nav__link">
    <span class="md-ellipsis">
      TensorRT framework
    </span>
  </a>
  
    <nav class="md-nav" aria-label="TensorRT framework">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#cuda-device_5" class="md-nav__link">
    <span class="md-ellipsis">
      CUDA device
    </span>
  </a>
  
    <nav class="md-nav" aria-label="CUDA device">
      <ul class="md-nav__list">
        
          <li class="md-nav__item">
  <a href="#docker-environment_11" class="md-nav__link">
    <span class="md-ellipsis">
      Docker Environment
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-estimation-for-offline-scenario_15" class="md-nav__link">
    <span class="md-ellipsis">
      Performance Estimation for Offline Scenario
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#offline_23" class="md-nav__link">
    <span class="md-ellipsis">
      Offline
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_46" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_46" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#server_15" class="md-nav__link">
    <span class="md-ellipsis">
      Server
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#performance-only_47" class="md-nav__link">
    <span class="md-ellipsis">
      performance-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#accuracy-only_47" class="md-nav__link">
    <span class="md-ellipsis">
      accuracy-only
    </span>
  </a>
  
</li>
        
          <li class="md-nav__item">
  <a href="#all-scenarios_23" class="md-nav__link">
    <span class="md-ellipsis">
      All Scenarios
    </span>
  </a>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
        
      </ul>
    </nav>
  
</li>
      
    </ul>
  
</nav>
                  </div>
                </div>
              </div>
            
          
          
            <div class="md-content" data-md-component="content">
              <article class="md-content__inner md-typeset">
                
                  
  



  
  


<h1 id="question-answering-using-bert-large">Question Answering using Bert-Large</h1>
<div class="tabbed-set tabbed-alternate" data-tabs="1:2"><input checked="checked" id="__tabbed_1_1" name="__tabbed_1" type="radio" /><input id="__tabbed_1_2" name="__tabbed_1" type="radio" /><div class="tabbed-labels"><label for="__tabbed_1_1">MLCommons-Python</label><label for="__tabbed_1_2">Nvidia</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h2 id="mlperf-reference-implementation-in-python">MLPerf Reference Implementation in Python</h2>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>MLCommons reference implementations are only meant to provide a rules compliant reference implementation for the submitters and in most cases are not best performing. If you want to benchmark any system, it is advisable to use the vendor MLPerf implementation for that system like Nvidia, Intel etc.</li>
</ul>
</div>
<p>BERT-99</p>
<div class="tabbed-set tabbed-alternate" data-tabs="2:2"><input checked="checked" id="__tabbed_2_1" name="__tabbed_2" type="radio" /><input id="__tabbed_2_2" name="__tabbed_2" type="radio" /><div class="tabbed-labels"><label for="__tabbed_2_1">datacenter</label><label for="__tabbed_2_2">edge</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h3 id="datacenter-category">Datacenter category</h3>
<p>In the datacenter category, bert-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="3:2"><input checked="checked" id="__tabbed_3_1" name="__tabbed_3" type="radio" /><input id="__tabbed_3_2" name="__tabbed_3" type="radio" /><div class="tabbed-labels"><label for="__tabbed_3_1">Pytorch</label><label for="__tabbed_3_2">Deepsparse</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h4 id="pytorch-framework">Pytorch framework</h4>
<div class="tabbed-set tabbed-alternate" data-tabs="4:3"><input checked="checked" id="__tabbed_4_1" name="__tabbed_4" type="radio" /><input id="__tabbed_4_2" name="__tabbed_4" type="radio" /><input id="__tabbed_4_3" name="__tabbed_4" type="radio" /><div class="tabbed-labels"><label for="__tabbed_4_1">CPU</label><label for="__tabbed_4_2">CUDA</label><label for="__tabbed_4_3">ROCm</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h5 id="cpu-device">CPU device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Disk Space</strong>: 50GB</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="5:2"><input checked="checked" id="__tabbed_5_1" name="__tabbed_5" type="radio" /><input id="__tabbed_5_2" name="__tabbed_5" type="radio" /><div class="tabbed-labels"><label for="__tabbed_5_1">Docker</label><label for="__tabbed_5_2">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="docker-container-build-and-performance-estimation-for-offline-scenario"># Docker Container Build and Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p>
<p><details>
<summary> Please click here to see more options for the docker launch </summary></p>
<ul>
<li>
<p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_cache=no</code>: to not use docker cache during the image build</p>
</li>
<li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li>
<li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL
</details></li>
</ul>
<div class="tabbed-set tabbed-alternate" data-tabs="6:3"><input checked="checked" id="__tabbed_6_1" name="__tabbed_6" type="radio" /><input id="__tabbed_6_2" name="__tabbed_6" type="radio" /><input id="__tabbed_6_3" name="__tabbed_6" type="radio" /><div class="tabbed-labels"><label for="__tabbed_6_1">Offline</label><label for="__tabbed_6_2">Server</label><label for="__tabbed_6_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="7:2"><input checked="checked" id="__tabbed_7_1" name="__tabbed_7" type="radio" /><input id="__tabbed_7_2" name="__tabbed_7" type="radio" /><div class="tabbed-labels"><label for="__tabbed_7_1">performance-only</label><label for="__tabbed_7_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="8:2"><input checked="checked" id="__tabbed_8_1" name="__tabbed_8" type="radio" /><input id="__tabbed_8_2" name="__tabbed_8" type="radio" /><div class="tabbed-labels"><label for="__tabbed_8_1">performance-only</label><label for="__tabbed_8_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_1">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_1">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
<div class="tabbed-block">
<h6 id="native-environment">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="setup-a-virtual-environment-for-python"># Setup a virtual environment for Python</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
<h6 id="performance-estimation-for-offline-scenario"># Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="9:3"><input checked="checked" id="__tabbed_9_1" name="__tabbed_9" type="radio" /><input id="__tabbed_9_2" name="__tabbed_9" type="radio" /><input id="__tabbed_9_3" name="__tabbed_9" type="radio" /><div class="tabbed-labels"><label for="__tabbed_9_1">Offline</label><label for="__tabbed_9_2">Server</label><label for="__tabbed_9_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_1">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="10:2"><input checked="checked" id="__tabbed_10_1" name="__tabbed_10" type="radio" /><input id="__tabbed_10_2" name="__tabbed_10" type="radio" /><div class="tabbed-labels"><label for="__tabbed_10_1">performance-only</label><label for="__tabbed_10_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_2">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_2">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_1">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="11:2"><input checked="checked" id="__tabbed_11_1" name="__tabbed_11" type="radio" /><input id="__tabbed_11_2" name="__tabbed_11" type="radio" /><div class="tabbed-labels"><label for="__tabbed_11_1">performance-only</label><label for="__tabbed_11_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_3">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_3">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_1">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h5 id="cuda-device">CUDA device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li>
<p><strong>Device Memory</strong>: To be updated</p>
</li>
<li>
<p><strong>Disk Space</strong>: 50GB</p>
</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="12:2"><input checked="checked" id="__tabbed_12_1" name="__tabbed_12" type="radio" /><input id="__tabbed_12_2" name="__tabbed_12" type="radio" /><div class="tabbed-labels"><label for="__tabbed_12_1">Docker</label><label for="__tabbed_12_2">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_1">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="docker-container-build-and-performance-estimation-for-offline-scenario_1"># Docker Container Build and Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">500</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p>
<p><details>
<summary> Please click here to see more options for the docker launch </summary></p>
<ul>
<li>
<p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_cache=no</code>: to not use docker cache during the image build
</details></p>
</li>
</ul>
<div class="tabbed-set tabbed-alternate" data-tabs="13:3"><input checked="checked" id="__tabbed_13_1" name="__tabbed_13" type="radio" /><input id="__tabbed_13_2" name="__tabbed_13" type="radio" /><input id="__tabbed_13_3" name="__tabbed_13" type="radio" /><div class="tabbed-labels"><label for="__tabbed_13_1">Offline</label><label for="__tabbed_13_2">Server</label><label for="__tabbed_13_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_2">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="14:2"><input checked="checked" id="__tabbed_14_1" name="__tabbed_14" type="radio" /><input id="__tabbed_14_2" name="__tabbed_14" type="radio" /><div class="tabbed-labels"><label for="__tabbed_14_1">performance-only</label><label for="__tabbed_14_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_4">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_4">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_2">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="15:2"><input checked="checked" id="__tabbed_15_1" name="__tabbed_15" type="radio" /><input id="__tabbed_15_2" name="__tabbed_15" type="radio" /><div class="tabbed-labels"><label for="__tabbed_15_1">performance-only</label><label for="__tabbed_15_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_5">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_5">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_2">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
<div class="tabbed-block">
<h6 id="native-environment_1">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li>
</ul>
</div>
<h6 id="setup-a-virtual-environment-for-python_1"># Setup a virtual environment for Python</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
<h6 id="performance-estimation-for-offline-scenario_1"># Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">500</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="16:3"><input checked="checked" id="__tabbed_16_1" name="__tabbed_16" type="radio" /><input id="__tabbed_16_2" name="__tabbed_16" type="radio" /><input id="__tabbed_16_3" name="__tabbed_16" type="radio" /><div class="tabbed-labels"><label for="__tabbed_16_1">Offline</label><label for="__tabbed_16_2">Server</label><label for="__tabbed_16_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_3">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="17:2"><input checked="checked" id="__tabbed_17_1" name="__tabbed_17" type="radio" /><input id="__tabbed_17_2" name="__tabbed_17" type="radio" /><div class="tabbed-labels"><label for="__tabbed_17_1">performance-only</label><label for="__tabbed_17_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_6">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_6">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_3">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="18:2"><input checked="checked" id="__tabbed_18_1" name="__tabbed_18" type="radio" /><input id="__tabbed_18_2" name="__tabbed_18" type="radio" /><div class="tabbed-labels"><label for="__tabbed_18_1">performance-only</label><label for="__tabbed_18_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_7">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_7">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_3">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h5 id="rocm-device">ROCm device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Disk Space</strong>: 50GB</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="19:1"><input checked="checked" id="__tabbed_19_1" name="__tabbed_19" type="radio" /><div class="tabbed-labels"><label for="__tabbed_19_1">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="native-environment_2">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="setup-a-virtual-environment-for-python_2"># Setup a virtual environment for Python</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
<h6 id="performance-estimation-for-offline-scenario_2"># Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="20:3"><input checked="checked" id="__tabbed_20_1" name="__tabbed_20" type="radio" /><input id="__tabbed_20_2" name="__tabbed_20" type="radio" /><input id="__tabbed_20_3" name="__tabbed_20" type="radio" /><div class="tabbed-labels"><label for="__tabbed_20_1">Offline</label><label for="__tabbed_20_2">Server</label><label for="__tabbed_20_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_4">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="21:2"><input checked="checked" id="__tabbed_21_1" name="__tabbed_21" type="radio" /><input id="__tabbed_21_2" name="__tabbed_21" type="radio" /><div class="tabbed-labels"><label for="__tabbed_21_1">performance-only</label><label for="__tabbed_21_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_8">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_8">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_4">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="22:2"><input checked="checked" id="__tabbed_22_1" name="__tabbed_22" type="radio" /><input id="__tabbed_22_2" name="__tabbed_22" type="radio" /><div class="tabbed-labels"><label for="__tabbed_22_1">performance-only</label><label for="__tabbed_22_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_9">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_9">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_4">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h4 id="deepsparse-framework">Deepsparse framework</h4>
<div class="tabbed-set tabbed-alternate" data-tabs="23:1"><input checked="checked" id="__tabbed_23_1" name="__tabbed_23" type="radio" /><div class="tabbed-labels"><label for="__tabbed_23_1">CPU</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h5 id="cpu-device_1">CPU device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Disk Space</strong>: 50GB</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="24:2"><input checked="checked" id="__tabbed_24_1" name="__tabbed_24" type="radio" /><input id="__tabbed_24_2" name="__tabbed_24" type="radio" /><div class="tabbed-labels"><label for="__tabbed_24_1">Docker</label><label for="__tabbed_24_2">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_2">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="docker-container-build-and-performance-estimation-for-offline-scenario_2"># Docker Container Build and Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>--rerun
</code></pre></div>
The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p>
<p><details>
<summary> Please click here to see more options for the docker launch </summary></p>
<ul>
<li>
<p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_cache=no</code>: to not use docker cache during the image build</p>
</li>
<li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li>
<li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL
</details></li>
</ul>
<div class="tabbed-set tabbed-alternate" data-tabs="25:3"><input checked="checked" id="__tabbed_25_1" name="__tabbed_25" type="radio" /><input id="__tabbed_25_2" name="__tabbed_25" type="radio" /><input id="__tabbed_25_3" name="__tabbed_25" type="radio" /><div class="tabbed-labels"><label for="__tabbed_25_1">Offline</label><label for="__tabbed_25_2">Server</label><label for="__tabbed_25_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_5">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="26:2"><input checked="checked" id="__tabbed_26_1" name="__tabbed_26" type="radio" /><input id="__tabbed_26_2" name="__tabbed_26" type="radio" /><div class="tabbed-labels"><label for="__tabbed_26_1">performance-only</label><label for="__tabbed_26_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_10">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_10">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_5">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="27:2"><input checked="checked" id="__tabbed_27_1" name="__tabbed_27" type="radio" /><input id="__tabbed_27_2" name="__tabbed_27" type="radio" /><div class="tabbed-labels"><label for="__tabbed_27_1">performance-only</label><label for="__tabbed_27_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_11">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_11">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_5">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
<p><details>
<summary> Please click here to view available generic model stubs for bert deepsparse</summary></p>
<ul>
<li>
<p><strong>obert-large-pruned95_quant-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50_quant-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-base_quant-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned95_obs_quant-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p>
</li>
<li>
<p><strong>obert-base-pruned90-none:</strong> zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>obert-large-pruned97_quant-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned90-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>bert-large-pruned80_quant-none-vnni:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned95-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned97-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p>
</li>
<li>
<p><strong>bert-large-base-none:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>obert-large-base-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>mobilebert-none-base-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none
</details></p>
</li>
</ul>
</div>
<div class="tabbed-block">
<h6 id="native-environment_3">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="setup-a-virtual-environment-for-python_3"># Setup a virtual environment for Python</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
<h6 id="performance-estimation-for-offline-scenario_3"># Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="28:3"><input checked="checked" id="__tabbed_28_1" name="__tabbed_28" type="radio" /><input id="__tabbed_28_2" name="__tabbed_28" type="radio" /><input id="__tabbed_28_3" name="__tabbed_28" type="radio" /><div class="tabbed-labels"><label for="__tabbed_28_1">Offline</label><label for="__tabbed_28_2">Server</label><label for="__tabbed_28_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_6">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="29:2"><input checked="checked" id="__tabbed_29_1" name="__tabbed_29" type="radio" /><input id="__tabbed_29_2" name="__tabbed_29" type="radio" /><div class="tabbed-labels"><label for="__tabbed_29_1">performance-only</label><label for="__tabbed_29_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_12">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_12">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_6">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="30:2"><input checked="checked" id="__tabbed_30_1" name="__tabbed_30" type="radio" /><input id="__tabbed_30_2" name="__tabbed_30" type="radio" /><div class="tabbed-labels"><label for="__tabbed_30_1">performance-only</label><label for="__tabbed_30_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_13">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_13">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_6">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
<p><details>
<summary> Please click here to view available generic model stubs for bert deepsparse</summary></p>
<ul>
<li>
<p><strong>obert-large-pruned95_quant-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50_quant-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-base_quant-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned95_obs_quant-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p>
</li>
<li>
<p><strong>obert-base-pruned90-none:</strong> zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>obert-large-pruned97_quant-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned90-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>bert-large-pruned80_quant-none-vnni:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned95-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned97-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p>
</li>
<li>
<p><strong>bert-large-base-none:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>obert-large-base-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>mobilebert-none-base-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none
</details></p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h3 id="edge-category">Edge category</h3>
<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="31:2"><input checked="checked" id="__tabbed_31_1" name="__tabbed_31" type="radio" /><input id="__tabbed_31_2" name="__tabbed_31" type="radio" /><div class="tabbed-labels"><label for="__tabbed_31_1">Pytorch</label><label for="__tabbed_31_2">Deepsparse</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h4 id="pytorch-framework_1">Pytorch framework</h4>
<div class="tabbed-set tabbed-alternate" data-tabs="32:3"><input checked="checked" id="__tabbed_32_1" name="__tabbed_32" type="radio" /><input id="__tabbed_32_2" name="__tabbed_32" type="radio" /><input id="__tabbed_32_3" name="__tabbed_32" type="radio" /><div class="tabbed-labels"><label for="__tabbed_32_1">CPU</label><label for="__tabbed_32_2">CUDA</label><label for="__tabbed_32_3">ROCm</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h5 id="cpu-device_2">CPU device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Disk Space</strong>: 50GB</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="33:2"><input checked="checked" id="__tabbed_33_1" name="__tabbed_33" type="radio" /><input id="__tabbed_33_2" name="__tabbed_33" type="radio" /><div class="tabbed-labels"><label for="__tabbed_33_1">Docker</label><label for="__tabbed_33_2">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_3">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="docker-container-build-and-performance-estimation-for-offline-scenario_3"># Docker Container Build and Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p>
<p><details>
<summary> Please click here to see more options for the docker launch </summary></p>
<ul>
<li>
<p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_cache=no</code>: to not use docker cache during the image build</p>
</li>
<li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li>
<li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL
</details></li>
</ul>
<div class="tabbed-set tabbed-alternate" data-tabs="34:3"><input checked="checked" id="__tabbed_34_1" name="__tabbed_34" type="radio" /><input id="__tabbed_34_2" name="__tabbed_34" type="radio" /><input id="__tabbed_34_3" name="__tabbed_34" type="radio" /><div class="tabbed-labels"><label for="__tabbed_34_1">Offline</label><label for="__tabbed_34_2">SingleStream</label><label for="__tabbed_34_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_7">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="35:2"><input checked="checked" id="__tabbed_35_1" name="__tabbed_35" type="radio" /><input id="__tabbed_35_2" name="__tabbed_35" type="radio" /><div class="tabbed-labels"><label for="__tabbed_35_1">performance-only</label><label for="__tabbed_35_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_14">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_14">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="singlestream">SingleStream</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="36:2"><input checked="checked" id="__tabbed_36_1" name="__tabbed_36" type="radio" /><input id="__tabbed_36_2" name="__tabbed_36" type="radio" /><div class="tabbed-labels"><label for="__tabbed_36_1">performance-only</label><label for="__tabbed_36_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_15">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_15">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_7">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
<div class="tabbed-block">
<h6 id="native-environment_4">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="setup-a-virtual-environment-for-python_4"># Setup a virtual environment for Python</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
<h6 id="performance-estimation-for-offline-scenario_4"># Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="37:3"><input checked="checked" id="__tabbed_37_1" name="__tabbed_37" type="radio" /><input id="__tabbed_37_2" name="__tabbed_37" type="radio" /><input id="__tabbed_37_3" name="__tabbed_37" type="radio" /><div class="tabbed-labels"><label for="__tabbed_37_1">Offline</label><label for="__tabbed_37_2">SingleStream</label><label for="__tabbed_37_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_8">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="38:2"><input checked="checked" id="__tabbed_38_1" name="__tabbed_38" type="radio" /><input id="__tabbed_38_2" name="__tabbed_38" type="radio" /><div class="tabbed-labels"><label for="__tabbed_38_1">performance-only</label><label for="__tabbed_38_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_16">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_16">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="singlestream_1">SingleStream</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="39:2"><input checked="checked" id="__tabbed_39_1" name="__tabbed_39" type="radio" /><input id="__tabbed_39_2" name="__tabbed_39" type="radio" /><div class="tabbed-labels"><label for="__tabbed_39_1">performance-only</label><label for="__tabbed_39_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_17">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_17">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_8">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h5 id="cuda-device_1">CUDA device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li>
<p><strong>Device Memory</strong>: To be updated</p>
</li>
<li>
<p><strong>Disk Space</strong>: 50GB</p>
</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="40:2"><input checked="checked" id="__tabbed_40_1" name="__tabbed_40" type="radio" /><input id="__tabbed_40_2" name="__tabbed_40" type="radio" /><div class="tabbed-labels"><label for="__tabbed_40_1">Docker</label><label for="__tabbed_40_2">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_4">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="docker-container-build-and-performance-estimation-for-offline-scenario_4"># Docker Container Build and Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">500</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p>
<p><details>
<summary> Please click here to see more options for the docker launch </summary></p>
<ul>
<li>
<p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_cache=no</code>: to not use docker cache during the image build
</details></p>
</li>
</ul>
<div class="tabbed-set tabbed-alternate" data-tabs="41:3"><input checked="checked" id="__tabbed_41_1" name="__tabbed_41" type="radio" /><input id="__tabbed_41_2" name="__tabbed_41" type="radio" /><input id="__tabbed_41_3" name="__tabbed_41" type="radio" /><div class="tabbed-labels"><label for="__tabbed_41_1">Offline</label><label for="__tabbed_41_2">SingleStream</label><label for="__tabbed_41_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_9">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="42:2"><input checked="checked" id="__tabbed_42_1" name="__tabbed_42" type="radio" /><input id="__tabbed_42_2" name="__tabbed_42" type="radio" /><div class="tabbed-labels"><label for="__tabbed_42_1">performance-only</label><label for="__tabbed_42_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_18">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_18">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="singlestream_2">SingleStream</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="43:2"><input checked="checked" id="__tabbed_43_1" name="__tabbed_43" type="radio" /><input id="__tabbed_43_2" name="__tabbed_43" type="radio" /><div class="tabbed-labels"><label for="__tabbed_43_1">performance-only</label><label for="__tabbed_43_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_19">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_19">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_9">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
<div class="tabbed-block">
<h6 id="native-environment_5">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li>
</ul>
</div>
<h6 id="setup-a-virtual-environment-for-python_5"># Setup a virtual environment for Python</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
<h6 id="performance-estimation-for-offline-scenario_5"># Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">500</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="44:3"><input checked="checked" id="__tabbed_44_1" name="__tabbed_44" type="radio" /><input id="__tabbed_44_2" name="__tabbed_44" type="radio" /><input id="__tabbed_44_3" name="__tabbed_44" type="radio" /><div class="tabbed-labels"><label for="__tabbed_44_1">Offline</label><label for="__tabbed_44_2">SingleStream</label><label for="__tabbed_44_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_10">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="45:2"><input checked="checked" id="__tabbed_45_1" name="__tabbed_45" type="radio" /><input id="__tabbed_45_2" name="__tabbed_45" type="radio" /><div class="tabbed-labels"><label for="__tabbed_45_1">performance-only</label><label for="__tabbed_45_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_20">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_20">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="singlestream_3">SingleStream</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="46:2"><input checked="checked" id="__tabbed_46_1" name="__tabbed_46" type="radio" /><input id="__tabbed_46_2" name="__tabbed_46" type="radio" /><div class="tabbed-labels"><label for="__tabbed_46_1">performance-only</label><label for="__tabbed_46_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_21">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_21">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_10">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h5 id="rocm-device_1">ROCm device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Disk Space</strong>: 50GB</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="47:1"><input checked="checked" id="__tabbed_47_1" name="__tabbed_47" type="radio" /><div class="tabbed-labels"><label for="__tabbed_47_1">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="native-environment_6">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="setup-a-virtual-environment-for-python_6"># Setup a virtual environment for Python</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
<h6 id="performance-estimation-for-offline-scenario_6"># Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="48:3"><input checked="checked" id="__tabbed_48_1" name="__tabbed_48" type="radio" /><input id="__tabbed_48_2" name="__tabbed_48" type="radio" /><input id="__tabbed_48_3" name="__tabbed_48" type="radio" /><div class="tabbed-labels"><label for="__tabbed_48_1">Offline</label><label for="__tabbed_48_2">SingleStream</label><label for="__tabbed_48_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_11">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="49:2"><input checked="checked" id="__tabbed_49_1" name="__tabbed_49" type="radio" /><input id="__tabbed_49_2" name="__tabbed_49" type="radio" /><div class="tabbed-labels"><label for="__tabbed_49_1">performance-only</label><label for="__tabbed_49_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_22">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_22">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="singlestream_4">SingleStream</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="50:2"><input checked="checked" id="__tabbed_50_1" name="__tabbed_50" type="radio" /><input id="__tabbed_50_2" name="__tabbed_50" type="radio" /><div class="tabbed-labels"><label for="__tabbed_50_1">performance-only</label><label for="__tabbed_50_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_23">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_23">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_11">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h4 id="deepsparse-framework_1">Deepsparse framework</h4>
<div class="tabbed-set tabbed-alternate" data-tabs="51:1"><input checked="checked" id="__tabbed_51_1" name="__tabbed_51" type="radio" /><div class="tabbed-labels"><label for="__tabbed_51_1">CPU</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h5 id="cpu-device_3">CPU device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Disk Space</strong>: 50GB</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="52:2"><input checked="checked" id="__tabbed_52_1" name="__tabbed_52" type="radio" /><input id="__tabbed_52_2" name="__tabbed_52" type="radio" /><div class="tabbed-labels"><label for="__tabbed_52_1">Docker</label><label for="__tabbed_52_2">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_5">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="docker-container-build-and-performance-estimation-for-offline-scenario_5"># Docker Container Build and Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>--rerun
</code></pre></div>
The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p>
<p><details>
<summary> Please click here to see more options for the docker launch </summary></p>
<ul>
<li>
<p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_cache=no</code>: to not use docker cache during the image build</p>
</li>
<li><code>--docker_os=ubuntu</code>: ubuntu and rhel are supported. </li>
<li><code>--docker_os_version=20.04</code>: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL
</details></li>
</ul>
<div class="tabbed-set tabbed-alternate" data-tabs="53:3"><input checked="checked" id="__tabbed_53_1" name="__tabbed_53" type="radio" /><input id="__tabbed_53_2" name="__tabbed_53" type="radio" /><input id="__tabbed_53_3" name="__tabbed_53" type="radio" /><div class="tabbed-labels"><label for="__tabbed_53_1">Offline</label><label for="__tabbed_53_2">SingleStream</label><label for="__tabbed_53_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_12">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="54:2"><input checked="checked" id="__tabbed_54_1" name="__tabbed_54" type="radio" /><input id="__tabbed_54_2" name="__tabbed_54" type="radio" /><div class="tabbed-labels"><label for="__tabbed_54_1">performance-only</label><label for="__tabbed_54_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_24">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_24">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="singlestream_5">SingleStream</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="55:2"><input checked="checked" id="__tabbed_55_1" name="__tabbed_55" type="radio" /><input id="__tabbed_55_2" name="__tabbed_55" type="radio" /><div class="tabbed-labels"><label for="__tabbed_55_1">performance-only</label><label for="__tabbed_55_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_25">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_25">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_12">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
<p><details>
<summary> Please click here to view available generic model stubs for bert deepsparse</summary></p>
<ul>
<li>
<p><strong>obert-large-pruned95_quant-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50_quant-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-base_quant-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned95_obs_quant-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p>
</li>
<li>
<p><strong>obert-base-pruned90-none:</strong> zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>obert-large-pruned97_quant-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned90-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>bert-large-pruned80_quant-none-vnni:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned95-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned97-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p>
</li>
<li>
<p><strong>bert-large-base-none:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>obert-large-base-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>mobilebert-none-base-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none
</details></p>
</li>
</ul>
</div>
<div class="tabbed-block">
<h6 id="native-environment_7">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="setup-a-virtual-environment-for-python_7"># Setup a virtual environment for Python</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
<h6 id="performance-estimation-for-offline-scenario_7"># Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
<li>
<p><code>_r4.1-dev</code> could also be given instead of <code>_r5.0-dev</code> if you want to run the benchmark with the MLPerf version being 4.1.</p>
</li>
<li>
<p>Add <code>--adr.mlperf-implementation.tags=_branch.master,_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the official MLPerf Inference implementation in a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.tags=_repo.&lt;CUSTOM_INFERENCE_REPO_LINK&gt;</code> if you are modifying the model config accuracy script in the submission checker within a custom fork.</p>
</li>
<li>
<p>Add <code>--adr.inference-src.version=custom</code> if you are using the modified MLPerf Inference code or accuracy script on submission checker within a custom fork.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="56:3"><input checked="checked" id="__tabbed_56_1" name="__tabbed_56" type="radio" /><input id="__tabbed_56_2" name="__tabbed_56" type="radio" /><input id="__tabbed_56_3" name="__tabbed_56" type="radio" /><div class="tabbed-labels"><label for="__tabbed_56_1">Offline</label><label for="__tabbed_56_2">SingleStream</label><label for="__tabbed_56_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_13">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="57:2"><input checked="checked" id="__tabbed_57_1" name="__tabbed_57" type="radio" /><input id="__tabbed_57_2" name="__tabbed_57" type="radio" /><div class="tabbed-labels"><label for="__tabbed_57_1">performance-only</label><label for="__tabbed_57_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_26">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_26">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="singlestream_6">SingleStream</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="58:2"><input checked="checked" id="__tabbed_58_1" name="__tabbed_58" type="radio" /><input id="__tabbed_58_2" name="__tabbed_58" type="radio" /><div class="tabbed-labels"><label for="__tabbed_58_1">performance-only</label><label for="__tabbed_58_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_27">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_27">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_13">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
<p><details>
<summary> Please click here to view available generic model stubs for bert deepsparse</summary></p>
<ul>
<li>
<p><strong>obert-large-pruned95_quant-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50_quant-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-base_quant-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned95_obs_quant-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p>
</li>
<li>
<p><strong>obert-base-pruned90-none:</strong> zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>obert-large-pruned97_quant-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned90-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>bert-large-pruned80_quant-none-vnni:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned95-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned97-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p>
</li>
<li>
<p><strong>bert-large-base-none:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>obert-large-base-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>mobilebert-none-base-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none
</details></p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>BERT-99.9</p>
<div class="tabbed-set tabbed-alternate" data-tabs="59:1"><input checked="checked" id="__tabbed_59_1" name="__tabbed_59" type="radio" /><div class="tabbed-labels"><label for="__tabbed_59_1">datacenter</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h3 id="datacenter-category_1">Datacenter category</h3>
<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="60:2"><input checked="checked" id="__tabbed_60_1" name="__tabbed_60" type="radio" /><input id="__tabbed_60_2" name="__tabbed_60" type="radio" /><div class="tabbed-labels"><label for="__tabbed_60_1">Pytorch</label><label for="__tabbed_60_2">Deepsparse</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h4 id="pytorch-framework_2">Pytorch framework</h4>
<div class="tabbed-set tabbed-alternate" data-tabs="61:3"><input checked="checked" id="__tabbed_61_1" name="__tabbed_61" type="radio" /><input id="__tabbed_61_2" name="__tabbed_61" type="radio" /><input id="__tabbed_61_3" name="__tabbed_61" type="radio" /><div class="tabbed-labels"><label for="__tabbed_61_1">CPU</label><label for="__tabbed_61_2">CUDA</label><label for="__tabbed_61_3">ROCm</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h5 id="cpu-device_4">CPU device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Disk Space</strong>: 50GB</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="62:2"><input checked="checked" id="__tabbed_62_1" name="__tabbed_62" type="radio" /><input id="__tabbed_62_2" name="__tabbed_62" type="radio" /><div class="tabbed-labels"><label for="__tabbed_62_1">Docker</label><label for="__tabbed_62_2">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_6">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<p>You can reuse the same environment as described for bert-99.</p>
<h6 id="performance-estimation-for-offline-scenario_8">Performance Estimation for Offline Scenario</h6>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="63:3"><input checked="checked" id="__tabbed_63_1" name="__tabbed_63" type="radio" /><input id="__tabbed_63_2" name="__tabbed_63" type="radio" /><input id="__tabbed_63_3" name="__tabbed_63" type="radio" /><div class="tabbed-labels"><label for="__tabbed_63_1">Offline</label><label for="__tabbed_63_2">Server</label><label for="__tabbed_63_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_14">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="64:2"><input checked="checked" id="__tabbed_64_1" name="__tabbed_64" type="radio" /><input id="__tabbed_64_2" name="__tabbed_64" type="radio" /><div class="tabbed-labels"><label for="__tabbed_64_1">performance-only</label><label for="__tabbed_64_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_28">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_28">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_7">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="65:2"><input checked="checked" id="__tabbed_65_1" name="__tabbed_65" type="radio" /><input id="__tabbed_65_2" name="__tabbed_65" type="radio" /><div class="tabbed-labels"><label for="__tabbed_65_1">performance-only</label><label for="__tabbed_65_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_29">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_29">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_14">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
<div class="tabbed-block">
<h6 id="native-environment_8">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<p>You can reuse the same environment as described for bert-99.</p>
<h6 id="performance-estimation-for-offline-scenario_9">Performance Estimation for Offline Scenario</h6>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="66:3"><input checked="checked" id="__tabbed_66_1" name="__tabbed_66" type="radio" /><input id="__tabbed_66_2" name="__tabbed_66" type="radio" /><input id="__tabbed_66_3" name="__tabbed_66" type="radio" /><div class="tabbed-labels"><label for="__tabbed_66_1">Offline</label><label for="__tabbed_66_2">Server</label><label for="__tabbed_66_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_15">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="67:2"><input checked="checked" id="__tabbed_67_1" name="__tabbed_67" type="radio" /><input id="__tabbed_67_2" name="__tabbed_67" type="radio" /><div class="tabbed-labels"><label for="__tabbed_67_1">performance-only</label><label for="__tabbed_67_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_30">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_30">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_8">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="68:2"><input checked="checked" id="__tabbed_68_1" name="__tabbed_68" type="radio" /><input id="__tabbed_68_2" name="__tabbed_68" type="radio" /><div class="tabbed-labels"><label for="__tabbed_68_1">performance-only</label><label for="__tabbed_68_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_31">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_31">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_15">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h5 id="cuda-device_2">CUDA device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li>
<p><strong>Device Memory</strong>: To be updated</p>
</li>
<li>
<p><strong>Disk Space</strong>: 50GB</p>
</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="69:2"><input checked="checked" id="__tabbed_69_1" name="__tabbed_69" type="radio" /><input id="__tabbed_69_2" name="__tabbed_69" type="radio" /><div class="tabbed-labels"><label for="__tabbed_69_1">Docker</label><label for="__tabbed_69_2">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_7">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<p>You can reuse the same environment as described for bert-99.</p>
<h6 id="performance-estimation-for-offline-scenario_10">Performance Estimation for Offline Scenario</h6>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">500</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="70:3"><input checked="checked" id="__tabbed_70_1" name="__tabbed_70" type="radio" /><input id="__tabbed_70_2" name="__tabbed_70" type="radio" /><input id="__tabbed_70_3" name="__tabbed_70" type="radio" /><div class="tabbed-labels"><label for="__tabbed_70_1">Offline</label><label for="__tabbed_70_2">Server</label><label for="__tabbed_70_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_16">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="71:2"><input checked="checked" id="__tabbed_71_1" name="__tabbed_71" type="radio" /><input id="__tabbed_71_2" name="__tabbed_71" type="radio" /><div class="tabbed-labels"><label for="__tabbed_71_1">performance-only</label><label for="__tabbed_71_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_32">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_32">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_9">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="72:2"><input checked="checked" id="__tabbed_72_1" name="__tabbed_72" type="radio" /><input id="__tabbed_72_2" name="__tabbed_72" type="radio" /><div class="tabbed-labels"><label for="__tabbed_72_1">performance-only</label><label for="__tabbed_72_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_33">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_33">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_16">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
<div class="tabbed-block">
<h6 id="native-environment_9">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>It is advisable to use the commands in the Docker tab for CUDA. Run the below native command only if you are already on a CUDA setup with cuDNN and TensorRT installed.</li>
</ul>
</div>
<p>You can reuse the same environment as described for bert-99.</p>
<h6 id="performance-estimation-for-offline-scenario_11">Performance Estimation for Offline Scenario</h6>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">500</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="73:3"><input checked="checked" id="__tabbed_73_1" name="__tabbed_73" type="radio" /><input id="__tabbed_73_2" name="__tabbed_73" type="radio" /><input id="__tabbed_73_3" name="__tabbed_73" type="radio" /><div class="tabbed-labels"><label for="__tabbed_73_1">Offline</label><label for="__tabbed_73_2">Server</label><label for="__tabbed_73_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_17">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="74:2"><input checked="checked" id="__tabbed_74_1" name="__tabbed_74" type="radio" /><input id="__tabbed_74_2" name="__tabbed_74" type="radio" /><div class="tabbed-labels"><label for="__tabbed_74_1">performance-only</label><label for="__tabbed_74_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_34">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_34">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_10">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="75:2"><input checked="checked" id="__tabbed_75_1" name="__tabbed_75" type="radio" /><input id="__tabbed_75_2" name="__tabbed_75" type="radio" /><div class="tabbed-labels"><label for="__tabbed_75_1">performance-only</label><label for="__tabbed_75_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_35">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_35">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_17">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h5 id="rocm-device_2">ROCm device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Disk Space</strong>: 50GB</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="76:1"><input checked="checked" id="__tabbed_76_1" name="__tabbed_76" type="radio" /><div class="tabbed-labels"><label for="__tabbed_76_1">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="native-environment_10">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<p>You can reuse the same environment as described for bert-99.</p>
<h6 id="performance-estimation-for-offline-scenario_12">Performance Estimation for Offline Scenario</h6>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="77:3"><input checked="checked" id="__tabbed_77_1" name="__tabbed_77" type="radio" /><input id="__tabbed_77_2" name="__tabbed_77" type="radio" /><input id="__tabbed_77_3" name="__tabbed_77" type="radio" /><div class="tabbed-labels"><label for="__tabbed_77_1">Offline</label><label for="__tabbed_77_2">Server</label><label for="__tabbed_77_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_18">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="78:2"><input checked="checked" id="__tabbed_78_1" name="__tabbed_78" type="radio" /><input id="__tabbed_78_2" name="__tabbed_78" type="radio" /><div class="tabbed-labels"><label for="__tabbed_78_1">performance-only</label><label for="__tabbed_78_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_36">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_36">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_11">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="79:2"><input checked="checked" id="__tabbed_79_1" name="__tabbed_79" type="radio" /><input id="__tabbed_79_2" name="__tabbed_79" type="radio" /><div class="tabbed-labels"><label for="__tabbed_79_1">performance-only</label><label for="__tabbed_79_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_37">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_37">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_18">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>rocm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h4 id="deepsparse-framework_2">Deepsparse framework</h4>
<div class="tabbed-set tabbed-alternate" data-tabs="80:1"><input checked="checked" id="__tabbed_80_1" name="__tabbed_80" type="radio" /><div class="tabbed-labels"><label for="__tabbed_80_1">CPU</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h5 id="cpu-device_5">CPU device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Disk Space</strong>: 50GB</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="81:2"><input checked="checked" id="__tabbed_81_1" name="__tabbed_81" type="radio" /><input id="__tabbed_81_2" name="__tabbed_81" type="radio" /><div class="tabbed-labels"><label for="__tabbed_81_1">Docker</label><label for="__tabbed_81_2">Native</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_8">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<p>You can reuse the same environment as described for bert-99.</p>
<h6 id="performance-estimation-for-offline-scenario_13">Performance Estimation for Offline Scenario</h6>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="82:3"><input checked="checked" id="__tabbed_82_1" name="__tabbed_82" type="radio" /><input id="__tabbed_82_2" name="__tabbed_82" type="radio" /><input id="__tabbed_82_3" name="__tabbed_82" type="radio" /><div class="tabbed-labels"><label for="__tabbed_82_1">Offline</label><label for="__tabbed_82_2">Server</label><label for="__tabbed_82_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_19">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="83:2"><input checked="checked" id="__tabbed_83_1" name="__tabbed_83" type="radio" /><input id="__tabbed_83_2" name="__tabbed_83" type="radio" /><div class="tabbed-labels"><label for="__tabbed_83_1">performance-only</label><label for="__tabbed_83_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_38">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_38">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_12">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="84:2"><input checked="checked" id="__tabbed_84_1" name="__tabbed_84" type="radio" /><input id="__tabbed_84_2" name="__tabbed_84" type="radio" /><div class="tabbed-labels"><label for="__tabbed_84_1">performance-only</label><label for="__tabbed_84_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_39">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_39">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_19">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
<p><details>
<summary> Please click here to view available generic model stubs for bert deepsparse</summary></p>
<ul>
<li>
<p><strong>obert-large-pruned95_quant-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50_quant-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-base_quant-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned95_obs_quant-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p>
</li>
<li>
<p><strong>obert-base-pruned90-none:</strong> zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>obert-large-pruned97_quant-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned90-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>bert-large-pruned80_quant-none-vnni:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned95-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned97-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p>
</li>
<li>
<p><strong>bert-large-base-none:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>obert-large-base-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>mobilebert-none-base-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none
</details></p>
</li>
</ul>
</div>
<div class="tabbed-block">
<h6 id="native-environment_11">Native Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<p>You can reuse the same environment as described for bert-99.</p>
<h6 id="performance-estimation-for-offline-scenario_14">Performance Estimation for Offline Scenario</h6>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.1-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="85:3"><input checked="checked" id="__tabbed_85_1" name="__tabbed_85" type="radio" /><input id="__tabbed_85_2" name="__tabbed_85" type="radio" /><input id="__tabbed_85_3" name="__tabbed_85" type="radio" /><div class="tabbed-labels"><label for="__tabbed_85_1">Offline</label><label for="__tabbed_85_2">Server</label><label for="__tabbed_85_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_20">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="86:2"><input checked="checked" id="__tabbed_86_1" name="__tabbed_86" type="radio" /><input id="__tabbed_86_2" name="__tabbed_86" type="radio" /><div class="tabbed-labels"><label for="__tabbed_86_1">performance-only</label><label for="__tabbed_86_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_40">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_40">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_13">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="87:2"><input checked="checked" id="__tabbed_87_1" name="__tabbed_87" type="radio" /><input id="__tabbed_87_2" name="__tabbed_87" type="radio" /><div class="tabbed-labels"><label for="__tabbed_87_1">performance-only</label><label for="__tabbed_87_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_41">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_41">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_20">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.1-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>reference<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>deepsparse<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--nm_model_zoo_stub<span class="o">=</span>zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none<span class="w"> </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful
</details></li>
</ul>
<p><details>
<summary> Please click here to view available generic model stubs for bert deepsparse</summary></p>
<ul>
<li>
<p><strong>obert-large-pruned95_quant-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50_quant-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50_quant-none-vnni</p>
</li>
<li>
<p><strong>mobilebert-none-base_quant-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned95_obs_quant-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned95_obs_quant-none</p>
</li>
<li>
<p><strong>mobilebert-none-14layer_pruned50-none-vnni:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/14layer_pruned50-none-vnni</p>
</li>
<li>
<p><strong>obert-base-pruned90-none:</strong> zoo:nlp/question_answering/obert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>obert-large-pruned97_quant-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97_quant-none</p>
</li>
<li>
<p><strong>bert-base-pruned90-none:</strong> zoo:nlp/question_answering/bert-base/pytorch/huggingface/squad/pruned90-none</p>
</li>
<li>
<p><strong>bert-large-pruned80_quant-none-vnni:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/pruned80_quant-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned95-none-vnni:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned95-none-vnni</p>
</li>
<li>
<p><strong>obert-large-pruned97-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/pruned97-none</p>
</li>
<li>
<p><strong>bert-large-base-none:</strong> zoo:nlp/question_answering/bert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>obert-large-base-none:</strong> zoo:nlp/question_answering/obert-large/pytorch/huggingface/squad/base-none</p>
</li>
<li>
<p><strong>mobilebert-none-base-none:</strong> zoo:nlp/question_answering/mobilebert-none/pytorch/huggingface/squad/base-none
</details></p>
</li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<ul>
<li>If you want to download the official MLPerf model and dataset for bert-99.9 you can follow <a href="../get-bert-data/">this README</a>.</li>
</ul>
</div>
<div class="tabbed-block">
<h2 id="nvidia-mlperf-implementation">Nvidia MLPerf Implementation</h2>
<p>BERT-99</p>
<div class="tabbed-set tabbed-alternate" data-tabs="88:2"><input checked="checked" id="__tabbed_88_1" name="__tabbed_88" type="radio" /><input id="__tabbed_88_2" name="__tabbed_88" type="radio" /><div class="tabbed-labels"><label for="__tabbed_88_1">datacenter</label><label for="__tabbed_88_2">edge</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h3 id="datacenter-category_2">Datacenter category</h3>
<p>In the datacenter category, bert-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="89:1"><input checked="checked" id="__tabbed_89_1" name="__tabbed_89" type="radio" /><div class="tabbed-labels"><label for="__tabbed_89_1">TensorRT</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h4 id="tensorrt-framework">TensorRT framework</h4>
<div class="tabbed-set tabbed-alternate" data-tabs="90:1"><input checked="checked" id="__tabbed_90_1" name="__tabbed_90" type="radio" /><div class="tabbed-labels"><label for="__tabbed_90_1">CUDA</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h5 id="cuda-device_3">CUDA device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Device Memory</strong>: To be updated</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="91:1"><input checked="checked" id="__tabbed_91_1" name="__tabbed_91" type="radio" /><div class="tabbed-labels"><label for="__tabbed_91_1">Docker</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_9">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="docker-container-build-and-performance-estimation-for-offline-scenario_6"># Docker Container Build and Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
</ul>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Default batch size is assigned based on <a href="https://github.com/mlcommons/cm4mlops/blob/dd0c35856969c68945524d5c80414c615f5fe42c/script/app-mlperf-inference-nvidia/_cm.yaml#L1129">GPU memory</a> or the <a href="https://github.com/mlcommons/cm4mlops/blob/dd0c35856969c68945524d5c80414c615f5fe42c/script/app-mlperf-inference-nvidia/_cm.yaml#L1370">specified GPU</a>. Please click more option for <em>docker launch</em> or <em>run command</em> to see how to specify the GPU name.</p>
</li>
<li>
<p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.0-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">500</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p>
<p><details>
<summary> Please click here to see more options for the docker launch </summary></p>
<ul>
<li>
<p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_cache=no</code>: to not use docker cache during the image build</p>
</li>
<li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used.
</details></li>
</ul>
<div class="tabbed-set tabbed-alternate" data-tabs="92:3"><input checked="checked" id="__tabbed_92_1" name="__tabbed_92" type="radio" /><input id="__tabbed_92_2" name="__tabbed_92" type="radio" /><input id="__tabbed_92_3" name="__tabbed_92" type="radio" /><div class="tabbed-labels"><label for="__tabbed_92_1">Offline</label><label for="__tabbed_92_2">Server</label><label for="__tabbed_92_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_21">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="93:2"><input checked="checked" id="__tabbed_93_1" name="__tabbed_93" type="radio" /><input id="__tabbed_93_2" name="__tabbed_93" type="radio" /><div class="tabbed-labels"><label for="__tabbed_93_1">performance-only</label><label for="__tabbed_93_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_42">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_42">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_14">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="94:2"><input checked="checked" id="__tabbed_94_1" name="__tabbed_94" type="radio" /><input id="__tabbed_94_2" name="__tabbed_94" type="radio" /><div class="tabbed-labels"><label for="__tabbed_94_1">performance-only</label><label for="__tabbed_94_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_43">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_43">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_21">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li>
<li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used.
</details></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h3 id="edge-category_1">Edge category</h3>
<p>In the edge category, bert-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="95:1"><input checked="checked" id="__tabbed_95_1" name="__tabbed_95" type="radio" /><div class="tabbed-labels"><label for="__tabbed_95_1">TensorRT</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h4 id="tensorrt-framework_1">TensorRT framework</h4>
<div class="tabbed-set tabbed-alternate" data-tabs="96:1"><input checked="checked" id="__tabbed_96_1" name="__tabbed_96" type="radio" /><div class="tabbed-labels"><label for="__tabbed_96_1">CUDA</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h5 id="cuda-device_4">CUDA device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Device Memory</strong>: To be updated</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="97:1"><input checked="checked" id="__tabbed_97_1" name="__tabbed_97" type="radio" /><div class="tabbed-labels"><label for="__tabbed_97_1">Docker</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_10">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<h6 id="docker-container-build-and-performance-estimation-for-offline-scenario_7"># Docker Container Build and Performance Estimation for Offline Scenario</h6>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Compliance runs can be enabled by adding <code>--compliance=yes</code>.</p>
</li>
<li>
<p>Number of threads could be adjusted using <code>--threads=#</code>, where <code>#</code> is the desired number of threads. This option works only if the implementation in use supports threading.</p>
</li>
<li>
<p>Batch size could be adjusted using <code>--batch_size=#</code>, where <code>#</code> is the desired batch size. This option works only if the implementation in use is supporting the given batch size.</p>
</li>
</ul>
</div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li>
<p>Default batch size is assigned based on <a href="https://github.com/mlcommons/cm4mlops/blob/dd0c35856969c68945524d5c80414c615f5fe42c/script/app-mlperf-inference-nvidia/_cm.yaml#L1129">GPU memory</a> or the <a href="https://github.com/mlcommons/cm4mlops/blob/dd0c35856969c68945524d5c80414c615f5fe42c/script/app-mlperf-inference-nvidia/_cm.yaml#L1370">specified GPU</a>. Please click more option for <em>docker launch</em> or <em>run command</em> to see how to specify the GPU name.</p>
</li>
<li>
<p>When run with <code>--all_models=yes</code>, all the benchmark models of NVIDIA implementation can be executed within the same container.</p>
</li>
</ul>
</div>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.0-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">500</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.</p>
<p><details>
<summary> Please click here to see more options for the docker launch </summary></p>
<ul>
<li>
<p><code>--docker_mlc_repo=&lt;Custom MLC GitHub repo URL in username@repo format&gt;</code>: to use a custom fork of cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_mlc_repo_branch=&lt;Custom MLC GitHub repo Branch&gt;</code>: to checkout a custom branch of the cloned cm4mlops repository inside the docker image</p>
</li>
<li>
<p><code>--docker_cache=no</code>: to not use docker cache during the image build</p>
</li>
<li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used.
</details></li>
</ul>
<div class="tabbed-set tabbed-alternate" data-tabs="98:3"><input checked="checked" id="__tabbed_98_1" name="__tabbed_98" type="radio" /><input id="__tabbed_98_2" name="__tabbed_98" type="radio" /><input id="__tabbed_98_3" name="__tabbed_98" type="radio" /><div class="tabbed-labels"><label for="__tabbed_98_1">Offline</label><label for="__tabbed_98_2">SingleStream</label><label for="__tabbed_98_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_22">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="99:2"><input checked="checked" id="__tabbed_99_1" name="__tabbed_99" type="radio" /><input id="__tabbed_99_2" name="__tabbed_99" type="radio" /><div class="tabbed-labels"><label for="__tabbed_99_1">performance-only</label><label for="__tabbed_99_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_44">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_44">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="singlestream_7">SingleStream</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="100:2"><input checked="checked" id="__tabbed_100_1" name="__tabbed_100" type="radio" /><input id="__tabbed_100_2" name="__tabbed_100" type="radio" /><div class="tabbed-labels"><label for="__tabbed_100_1">performance-only</label><label for="__tabbed_100_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_45">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_45">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_22">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li>
<li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used.
</details></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<p>BERT-99.9</p>
<div class="tabbed-set tabbed-alternate" data-tabs="101:1"><input checked="checked" id="__tabbed_101_1" name="__tabbed_101" type="radio" /><div class="tabbed-labels"><label for="__tabbed_101_1">datacenter</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h3 id="datacenter-category_3">Datacenter category</h3>
<p>In the datacenter category, bert-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="102:1"><input checked="checked" id="__tabbed_102_1" name="__tabbed_102" type="radio" /><div class="tabbed-labels"><label for="__tabbed_102_1">TensorRT</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h4 id="tensorrt-framework_2">TensorRT framework</h4>
<div class="tabbed-set tabbed-alternate" data-tabs="103:1"><input checked="checked" id="__tabbed_103_1" name="__tabbed_103" type="radio" /><div class="tabbed-labels"><label for="__tabbed_103_1">CUDA</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h5 id="cuda-device_5">CUDA device</h5>
<p><details>
<summary>Please click here to see the minimum system requirements for running the benchmark</summary></p>
<ul>
<li><strong>Device Memory</strong>: To be updated</li>
</ul>
</details>
<div class="tabbed-set tabbed-alternate" data-tabs="104:1"><input checked="checked" id="__tabbed_104_1" name="__tabbed_104" type="radio" /><div class="tabbed-labels"><label for="__tabbed_104_1">Docker</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="docker-environment_11">Docker Environment</h6>
<p>Please refer to the <a href="/inference/install/">installation page</a> to install MLCFlow for running the automated benchmark commands.</p>
<p>You can reuse the same environment as described for bert-99.</p>
<h6 id="performance-estimation-for-offline-scenario_15">Performance Estimation for Offline Scenario</h6>
<p><div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.0-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">500</span><span class="w"> </span>--rerun
</code></pre></div>
The above command should do a test run of Offline scenario and record the estimated offline_target_qps.</p>
<div class="tabbed-set tabbed-alternate" data-tabs="105:3"><input checked="checked" id="__tabbed_105_1" name="__tabbed_105" type="radio" /><input id="__tabbed_105_2" name="__tabbed_105" type="radio" /><input id="__tabbed_105_3" name="__tabbed_105" type="radio" /><div class="tabbed-labels"><label for="__tabbed_105_1">Offline</label><label for="__tabbed_105_2">Server</label><label for="__tabbed_105_3">All Scenarios</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="offline_23">Offline</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="106:2"><input checked="checked" id="__tabbed_106_1" name="__tabbed_106" type="radio" /><input id="__tabbed_106_2" name="__tabbed_106" type="radio" /><div class="tabbed-labels"><label for="__tabbed_106_1">performance-only</label><label for="__tabbed_106_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_46">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_46">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="server_15">Server</h6>
<div class="tabbed-set tabbed-alternate" data-tabs="107:2"><input checked="checked" id="__tabbed_107_1" name="__tabbed_107" type="radio" /><input id="__tabbed_107_2" name="__tabbed_107" type="radio" /><div class="tabbed-labels"><label for="__tabbed_107_1">performance-only</label><label for="__tabbed_107_2">accuracy-only</label></div>
<div class="tabbed-content">
<div class="tabbed-block">
<h6 id="performance-only_47">performance-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
<div class="tabbed-block">
<h6 id="accuracy-only_47">accuracy-only</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
</div>
<div class="tabbed-block">
<h6 id="all-scenarios_23">All Scenarios</h6>
<div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>nvidia<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>tensorrt<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cuda<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
<div class="admonition tip">
<p class="admonition-title">Tip</p>
<ul>
<li><code>&lt;SERVER_TARGET_QPS&gt;</code> must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.</li>
</ul>
</div>
</div>
</div>
</div>
<p><details>
<summary> Please click here to see more options for the RUN command</summary></p>
<ul>
<li>
<p>Use <code>--division=closed</code> to do a closed division submission which includes compliance runs</p>
</li>
<li>
<p>Use <code>--rerun</code> to do a rerun even when a valid run exists</p>
</li>
<li>Use <code>--compliance</code> to do the compliance runs (only applicable for closed division) once the valid runs are successful</li>
<li><code>--gpu_name=&lt;Name of the GPU&gt;</code> : The GPUs with supported configs in MLC are <code>orin</code>, <code>rtx_4090</code>, <code>rtx_a6000</code>, <code>rtx_6000_ada</code>, <code>l4</code>, <code>t4</code>and <code>a100</code>. For other GPUs, default configuration as per the GPU memory will be used.
</details></li>
</ul>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
</div>
<!-- 
=== "Intel"
    ## Intel MLPerf Implementation


    !!! tip

        - Intel MLPerf inference implementation is available only for datacenter category and has been tested only on a limited number of systems. Most of the benchmarks using Intel implementation require at least Intel Sapphire Rapids or higher CPU generation.

    BERT-99

    === "datacenter"

        ### Datacenter category 

         In the datacenter category, bert-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.

        === "Pytorch"
            #### Pytorch framework

            === "CPU"
                ##### CPU device

                === "Docker"
                    ###### Docker Environment

                    Please refer to the [installation page](site:install/) to install MLCFlow for running the automated benchmark commands.

                    ####### Docker Container Build and Performance Estimation for Offline Scenario

                    !!! tip

                        - Compliance runs can be enabled by adding `--compliance=yes`.

                        - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.

                        - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.



                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r4.0<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
                    The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.

                    <details>
                    <summary> Please click here to see more options for the docker launch </summary>

                    * `--docker_mlc_repo=<Custom MLC GitHub repo URL in username@repo format>`: to use a custom fork of cm4mlops repository inside the docker image

                    * `--docker_mlc_repo_branch=<Custom MLC GitHub repo Branch>`: to checkout a custom branch of the cloned cm4mlops repository inside the docker image

                    * `--docker_cache=no`: to not use docker cache during the image build
                    * `--docker_os=ubuntu`: ubuntu and rhel are supported. 
                    * `--docker_os_version=20.04`: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL
                    </details>
                    === "Offline"
                        ###### Offline

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "Server"
                        ###### Server

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    === "All Scenarios"
                        ###### All Scenarios



                        <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        !!! tip

                            * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    <details>
                    <summary> Please click here to see more options for the RUN command</summary>

                    * Use `--division=closed` to do a closed division submission which includes compliance runs

                    * Use `--rerun` to do a rerun even when a valid run exists
                    * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful
                    </details>

                === "Native"
                    ###### Native Environment

                    Please refer to the [installation page](site:install/) to install MLCFlow for running the automated benchmark commands.

                    ####### Setup a virtual environment for Python


                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
                    ####### Performance Estimation for Offline Scenario

                    !!! tip

                        - Compliance runs can be enabled by adding `--compliance=yes`.

                        - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.

                        - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.



                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r4.0<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
                    The above command should do a test run of Offline scenario and record the estimated offline_target_qps.

                    === "Offline"
                        ###### Offline

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "Server"
                        ###### Server

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    === "All Scenarios"
                        ###### All Scenarios



                        <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        !!! tip

                            * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    <details>
                    <summary> Please click here to see more options for the RUN command</summary>

                    * Use `--division=closed` to do a closed division submission which includes compliance runs

                    * Use `--rerun` to do a rerun even when a valid run exists
                    * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful
                    </details>

    === "edge"

        ### Edge category 

         In the edge category, bert-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.

        === "Pytorch"
            #### Pytorch framework

            === "CPU"
                ##### CPU device

                === "Docker"
                    ###### Docker Environment

                    Please refer to the [installation page](site:install/) to install MLCFlow for running the automated benchmark commands.

                    ####### Docker Container Build and Performance Estimation for Offline Scenario

                    !!! tip

                        - Compliance runs can be enabled by adding `--compliance=yes`.

                        - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.

                        - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.



                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r4.0<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--docker<span class="w"> </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
                    The above command should get you to an interactive shell inside the docker container and do a quick test run for the Offline scenario. Once inside the docker container please do the below commands to do the accuracy + performance runs for each scenario.

                    <details>
                    <summary> Please click here to see more options for the docker launch </summary>

                    * `--docker_mlc_repo=<Custom MLC GitHub repo URL in username@repo format>`: to use a custom fork of cm4mlops repository inside the docker image

                    * `--docker_mlc_repo_branch=<Custom MLC GitHub repo Branch>`: to checkout a custom branch of the cloned cm4mlops repository inside the docker image

                    * `--docker_cache=no`: to not use docker cache during the image build
                    * `--docker_os=ubuntu`: ubuntu and rhel are supported. 
                    * `--docker_os_version=20.04`: [20.04, 22.04] are supported for Ubuntu and [8, 9] for RHEL
                    </details>
                    === "Offline"
                        ###### Offline

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "SingleStream"
                        ###### SingleStream

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "All Scenarios"
                        ###### All Scenarios



                        <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    <details>
                    <summary> Please click here to see more options for the RUN command</summary>

                    * Use `--division=closed` to do a closed division submission which includes compliance runs

                    * Use `--rerun` to do a rerun even when a valid run exists
                    * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful
                    </details>

                === "Native"
                    ###### Native Environment

                    Please refer to the [installation page](site:install/) to install MLCFlow for running the automated benchmark commands.

                    ####### Setup a virtual environment for Python


                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
                    ####### Performance Estimation for Offline Scenario

                    !!! tip

                        - Compliance runs can be enabled by adding `--compliance=yes`.

                        - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.

                        - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.



                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r4.0<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
                    The above command should do a test run of Offline scenario and record the estimated offline_target_qps.

                    === "Offline"
                        ###### Offline

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "SingleStream"
                        ###### SingleStream

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "All Scenarios"
                        ###### All Scenarios



                        <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    <details>
                    <summary> Please click here to see more options for the RUN command</summary>

                    * Use `--division=closed` to do a closed division submission which includes compliance runs

                    * Use `--rerun` to do a rerun even when a valid run exists
                    * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful
                    </details>



    BERT-99.9

    === "datacenter"

        ### Datacenter category 

         In the datacenter category, bert-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.

        === "Pytorch"
            #### Pytorch framework

            === "CPU"
                ##### CPU device

                === "Docker"
                    ###### Docker Environment

                    Please refer to the [installation page](site:install/) to install MLCFlow for running the automated benchmark commands.

                     You can reuse the same environment as described for bert-99.
                    ###### Performance Estimation for Offline Scenario


                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r4.0<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
                    The above command should do a test run of Offline scenario and record the estimated offline_target_qps.

                    === "Offline"
                        ###### Offline

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "Server"
                        ###### Server

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    === "All Scenarios"
                        ###### All Scenarios



                        <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        !!! tip

                            * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    <details>
                    <summary> Please click here to see more options for the RUN command</summary>

                    * Use `--division=closed` to do a closed division submission which includes compliance runs

                    * Use `--rerun` to do a rerun even when a valid run exists
                    * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful
                    </details>

                === "Native"
                    ###### Native Environment

                    Please refer to the [installation page](site:install/) to install MLCFlow for running the automated benchmark commands.

                     You can reuse the same environment as described for bert-99.
                    ###### Performance Estimation for Offline Scenario


                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r4.0<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
                    The above command should do a test run of Offline scenario and record the estimated offline_target_qps.

                    === "Offline"
                        ###### Offline

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "Server"
                        ###### Server

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    === "All Scenarios"
                        ###### All Scenarios



                        <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r4.0,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>intel<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>pytorch<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>cpu<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        !!! tip

                            * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    <details>
                    <summary> Please click here to see more options for the RUN command</summary>

                    * Use `--division=closed` to do a closed division submission which includes compliance runs

                    * Use `--rerun` to do a rerun even when a valid run exists
                    * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful
                    </details>



=== "Qualcomm"
    ## Qualcomm AI100 MLPerf Implementation

    BERT-99

    === "datacenter"

        ### Datacenter category 

         In the datacenter category, bert-99 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.

        === "Glow"
            #### Glow framework

            === "QAIC"
                ##### QAIC device

                === "Native"
                    ###### Native Environment

                    Please refer to the [installation page](site:install/) to install MLCFlow for running the automated benchmark commands.

                    ####### Setup a virtual environment for Python


                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
                    ####### Performance Estimation for Offline Scenario

                    !!! tip

                        - Compliance runs can be enabled by adding `--compliance=yes`.

                        - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.

                        - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.



                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.0-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
                    The above command should do a test run of Offline scenario and record the estimated offline_target_qps.

                    === "Offline"
                        ###### Offline

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "Server"
                        ###### Server

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    === "All Scenarios"
                        ###### All Scenarios



                        <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        !!! tip

                            * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    <details>
                    <summary> Please click here to see more options for the RUN command</summary>

                    * Use `--division=closed` to do a closed division submission which includes compliance runs

                    * Use `--rerun` to do a rerun even when a valid run exists
                    * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful
                    </details>

    === "edge"

        ### Edge category 

         In the edge category, bert-99 has Offline, SingleStream scenarios and all of the scenarios are  mandatory for a closed division submission.

        === "Glow"
            #### Glow framework

            === "QAIC"
                ##### QAIC device

                === "Native"
                    ###### Native Environment

                    Please refer to the [installation page](site:install/) to install MLCFlow for running the automated benchmark commands.

                    ####### Setup a virtual environment for Python


                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>install,python-venv<span class="w"> </span>--name<span class="o">=</span>mlperf
<span class="nb">export</span><span class="w"> </span><span class="nv">MLC_SCRIPT_EXTRA_CMD</span><span class="o">=</span><span class="s2">&quot;--adr.python.name=mlperf&quot;</span>
</code></pre></div>
                    ####### Performance Estimation for Offline Scenario

                    !!! tip

                        - Compliance runs can be enabled by adding `--compliance=yes`.

                        - Number of threads could be adjusted using `--threads=#`, where `#` is the desired number of threads. This option works only if the implementation in use supports threading.

                        - Batch size could be adjusted using `--batch_size=#`, where `#` is the desired batch size. This option works only if the implementation in use is supporting the given batch size.



                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.0-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
                    The above command should do a test run of Offline scenario and record the estimated offline_target_qps.

                    === "Offline"
                        ###### Offline

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "SingleStream"
                        ###### SingleStream

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>SingleStream<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "All Scenarios"
                        ###### All Scenarios



                        <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>edge<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    <details>
                    <summary> Please click here to see more options for the RUN command</summary>

                    * Use `--division=closed` to do a closed division submission which includes compliance runs

                    * Use `--rerun` to do a rerun even when a valid run exists
                    * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful
                    </details>



    BERT-99.9

    === "datacenter"

        ### Datacenter category 

         In the datacenter category, bert-99.9 has Offline, Server scenarios and all of the scenarios are  mandatory for a closed division submission.

        === "Glow"
            #### Glow framework

            === "QAIC"
                ##### QAIC device

                === "Native"
                    ###### Native Environment

                    Please refer to the [installation page](site:install/) to install MLCFlow for running the automated benchmark commands.

                     You can reuse the same environment as described for bert-99.
                    ###### Performance Estimation for Offline Scenario


                    <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_find-performance,_full,_r5.0-dev<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span><span class="nb">test</span><span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w">  </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--test_query_count<span class="o">=</span><span class="m">100</span><span class="w"> </span>--rerun
</code></pre></div>
                    The above command should do a test run of Offline scenario and record the estimated offline_target_qps.

                    === "Offline"
                        ###### Offline

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Offline<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                    === "Server"
                        ###### Server

                        === "performance-only"
                            ###### performance-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_performance-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                        === "accuracy-only"
                            ###### accuracy-only



                            <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_accuracy-only<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--scenario<span class="o">=</span>Server<span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                            !!! tip

                                * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    === "All Scenarios"
                        ###### All Scenarios



                        <div class="highlight"><pre><span></span><code>mlcr<span class="w"> </span>run-mlperf,inference,_full,_r5.0-dev,_all-scenarios<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--model<span class="o">=</span>bert-99.9<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--implementation<span class="o">=</span>qualcomm<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--framework<span class="o">=</span>glow<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--category<span class="o">=</span>datacenter<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--server_target_qps<span class="o">=</span>&lt;SERVER_TARGET_QPS&gt;<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--execution_mode<span class="o">=</span>valid<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--device<span class="o">=</span>qaic<span class="w"> </span><span class="se">\</span>
<span class="w">   </span>--quiet<span class="w">  </span>
</code></pre></div>
                        !!! tip

                            * `<SERVER_TARGET_QPS>` must be determined manually. It is usually around 80% of the Offline QPS, but on some systems, it can drop below 50%. If a higher value is specified, the latency constraint will not be met, and the run will be considered invalid.
                    <details>
                    <summary> Please click here to see more options for the RUN command</summary>

                    * Use `--division=closed` to do a closed division submission which includes compliance runs

                    * Use `--rerun` to do a rerun even when a valid run exists
                    * Use `--compliance` to do the compliance runs (only applicable for closed division) once the valid runs are successful
                    </details>


 -->
<p>s</p>







  
  




  



                
              </article>
            </div>
          
          
  <script>var tabs=__md_get("__tabs");if(Array.isArray(tabs))e:for(var set of document.querySelectorAll(".tabbed-set")){var labels=set.querySelector(".tabbed-labels");for(var tab of tabs)for(var label of labels.getElementsByTagName("label"))if(label.innerText.trim()===tab){var input=document.getElementById(label.htmlFor);input.checked=!0;continue e}}</script>

<script>var target=document.getElementById(location.hash.slice(1));target&&target.name&&(target.checked=target.name.startsWith("__tabbed_"))</script>
        </div>
        
          <button type="button" class="md-top md-icon" data-md-component="top" hidden>
  
  <svg xmlns="http://www.w3.org/2000/svg" viewBox="0 0 24 24"><path d="M13 20h-2V8l-5.5 5.5-1.42-1.42L12 4.16l7.92 7.92-1.42 1.42L13 8z"/></svg>
  Back to top
</button>
        
      </main>
      
        <footer class="md-footer">
  
  <div class="md-footer-meta md-typeset">
    <div class="md-footer-meta__inner md-grid">
      <div class="md-copyright">
  
  
    Made with
    <a href="https://squidfunk.github.io/mkdocs-material/" target="_blank" rel="noopener">
      Material for MkDocs
    </a>
  
</div>
      
    </div>
  </div>
</footer>
      
    </div>
    <div class="md-dialog" data-md-component="dialog">
      <div class="md-dialog__inner md-typeset"></div>
    </div>
    
    
    
      
      <script id="__config" type="application/json">{"base": "../../..", "features": ["content.tabs.link", "content.code.copy", "navigation.expand", "navigation.indexes", "navigation.sections", "navigation.instant", "navigation.tabs", "navigation.path", "navigation.tabs.sticky", "navigation.top", "navigation.prune", "toc.follow"], "search": "../../../assets/javascripts/workers/search.973d3a69.min.js", "tags": null, "translations": {"clipboard.copied": "Copied to clipboard", "clipboard.copy": "Copy to clipboard", "search.result.more.one": "1 more on this page", "search.result.more.other": "# more on this page", "search.result.none": "No matching documents", "search.result.one": "1 matching document", "search.result.other": "# matching documents", "search.result.placeholder": "Type to start searching", "search.result.term.missing": "Missing", "select.version": "Select version"}, "version": null}</script>
    
    
      <script src="../../../assets/javascripts/bundle.f55a23d4.min.js"></script>
      
    
  </body>
</html>